{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshinharshi/huggingface_NLP/blob/main/transformers-course/chapter1/Behind_the_pipeline_(PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMaGjlc67saK"
      },
      "source": [
        "# Behind the pipeline (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNgKX_ma7saO"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load token\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "os.environ['HF_TOKEN']= os.environ.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline \n",
        "The most basic object in the HuggingFace Transformers library is the pipeline() function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer. pipeline() function groups together three steps: preprocessing, passing the inputs through the model, and postprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "pipeline(\"sentiment-analysis\") : if we mention the pipeline without the model name a default checkpoint is used, here distilbert/distilbert-base-uncased-finetuned-sst-2-english is used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "q3JQGgoO7saQ",
        "outputId": "1f6d80da-09bf-4af8-b914-d7352ce24bc7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.999572217464447},\n",
              " {'label': 'NEGATIVE', 'score': 0.9974052309989929}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "        \"I've love pizza\",\n",
        "        \"I hate ice cream\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Behind Pipeline\n",
        "Hugging Face allows us to bypass the high-level pipeline abstraction and directly utilize the tokenizer and model for more granular control. In this explanation, we’ll explore how to implement this approach, which involves two key steps:  \n",
        "\n",
        "1. **Preprocessing with a Tokenizer**  \n",
        "   This step involves converting raw text into tokenized inputs (e.g., token IDs, attention masks) that the model can process.  \n",
        "\n",
        "2. **Processing with the Model**  \n",
        "   Once preprocessed, the tokenized data is passed through the model to generate outputs, such as predictions or embeddings.  \n",
        "\n",
        "Let’s break down each step in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing with a tokenizer\n",
        "**Core Objective**: Transform raw text into numerical representations through the following steps:  \n",
        "\n",
        "1. **Tokenization**  \n",
        "   Split the input text into smaller units (tokens), such as words, subwords, or symbols (e.g., punctuation marks).  \n",
        "\n",
        "2. **Numerical Mapping**  \n",
        "   Convert each token into a corresponding integer ID using a predefined vocabulary or lookup table.  \n",
        "\n",
        "3. **Input Augmentation**  \n",
        "   Add special tokens (e.g., `[CLS]`, `[SEP]`) or metadata (e.g., attention masks, token type IDs) required by the model to process the input effectively.  \n",
        "\n",
        "This process ensures the text is structured into a format compatible with machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the default checkpoint of the sentiment-analysis pipeline is distilbert-base-uncased-finetuned-sst-2-english. We are selecting that model here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM8nRvD97saR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\projects\\Hugging_face_NLP\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\harsh\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# initializing tokenizer\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. raw_inputs: A string or a list of strings to be tokenized.\n",
        "2. padding=True: Pads shorter sequences to match the longest sequence in the batch.\n",
        "3. truncation=True: Truncates longer sequences to fit within the model's token limit.\n",
        "3. return_tensors=\"pt\": Returns the output as PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "joWag0er7saR",
        "outputId": "9e5b5a15-f115-4a52-a05b-779fbbfd972a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2293, 10733,   102],\n",
            "        [  101,  1045,  5223,  3256,  6949,   102,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 0]])}\n"
          ]
        }
      ],
      "source": [
        "raw_inputs = [\n",
        "        \"I've love pizza\",\n",
        "        \"I hate ice cream\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Going through the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
        "\n",
        "1. Batch size: The number of sequences processed at a time (2 in our example).\n",
        "2. Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
        "3. Hidden size: The vector dimension of each model input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "0BUG05oQ7saS"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "HhIQwn8t7saS",
        "outputId": "89212331-e9fa-4aae-b457-ae444f2a9198"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 7, 768])\n"
          ]
        }
      ],
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output shape torch.Size([2, 7, 768]) represents a 3D tensor with the following dimensions:\n",
        "\n",
        "2 → Batch size: The model processed two input sequences.\n",
        "\n",
        "7 → Sequence length: Each sequence contains 7 tokens (after tokenization, padding, or truncation).\n",
        "\n",
        "768 → Hidden size: Each token is represented by a 768-dimensional vector (common in models like BERT).\n",
        "\n",
        "Meaning:\n",
        "\n",
        "Each sequence has 7 tokens.\n",
        "\n",
        "Each token is encoded as a 768-dimensional vector (output from a transformer model's hidden layer).\n",
        "\n",
        "The batch contains 2 sequences.\n",
        "\n",
        "This output is typically from the last hidden layer of a transformer-based model like BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BaseModelOutput(last_hidden_state=tensor([[[ 0.2425, -0.1938,  0.2781,  ...,  0.4556,  1.1108, -0.2243],\n",
              "         [ 0.7231, -0.0086,  0.2829,  ...,  0.3625,  1.2168, -0.0976],\n",
              "         [ 1.1178, -0.0109,  0.4670,  ...,  0.7832,  0.2728, -0.7872],\n",
              "         ...,\n",
              "         [ 0.8897, -0.0912,  0.6215,  ...,  0.2053,  1.0163,  0.0544],\n",
              "         [ 0.2794, -0.2736,  0.5849,  ...,  0.5751,  0.7293, -0.1187],\n",
              "         [ 0.7754, -0.1281,  0.4774,  ...,  0.7552,  0.8777, -0.5446]],\n",
              "\n",
              "        [[-0.0709,  0.7972, -0.2863,  ..., -0.1594, -0.1840,  0.1574],\n",
              "         [-0.2289,  0.8642, -0.1575,  ..., -0.2345, -0.0336,  0.3861],\n",
              "         [-0.0514,  0.9576, -0.0853,  ..., -0.2791, -0.0349,  0.3560],\n",
              "         ...,\n",
              "         [-0.8821,  0.5989, -0.0820,  ...,  0.0766, -0.0486,  0.1625],\n",
              "         [ 0.1153,  0.3351, -0.0721,  ...,  0.1415, -0.1309, -0.1116],\n",
              "         [-0.1579,  0.5985, -0.1570,  ..., -0.0764, -0.0212,  0.0562]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won’t actually use the AutoModel class, but AutoModelForSequenceClassification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "uTBgEn3E7saT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now if we look at the shape of our outputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mcfxB9117saT",
        "outputId": "c1918a90-240e-452d-a174-a499032ab8cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2])\n"
          ]
        }
      ],
      "source": [
        "print(outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "dL19ZyB97saU",
        "outputId": "5b8b4d16-5762-475a-d63f-88fed8b79974"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-3.7717,  3.9848],\n",
            "        [ 3.2460, -2.7057]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To be converted to probabilities, they need to go through a SoftMax layer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qknDzneb7saU",
        "outputId": "be5bfd19-06e0-47c3-a7ca-bf7b4f2ed887"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[4.2779e-04, 9.9957e-01],\n",
            "        [9.9741e-01, 2.5947e-03]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jc1DX-M_7saU",
        "outputId": "2640f36b-0a92-4307-8267-31f7b1e783ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'NEGATIVE', 1: 'POSITIVE'}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.id2label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can conclude that the model predicted the following:\n",
        "\n",
        "First sentence: NEGATIVE: 0.0004277, POSITIVE: 0.99957\n",
        "\n",
        "Second sentence: NEGATIVE: 0.99741, POSITIVE: 0.000259"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "Behind the pipeline (PyTorch)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
